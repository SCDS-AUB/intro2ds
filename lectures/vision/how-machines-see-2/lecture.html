---
layout: default
title: "How Machines See - Part 2: Modern Machine Learning"
---

<nav class="breadcrumb">
  <a href="{{ '/' | relative_url }}">Home</a> /
  <a href="{{ '/lectures.html' | relative_url }}">Lectures</a> /
  <a href="{{ '/lectures.html#vision' | relative_url }}">Vision</a> /
  Part 2
</nav>

<article class="lecture-content">
  <header>
    <h1>How Machines See - Part 2</h1>
    <p class="subtitle">Modern Machine Learning</p>
    <div class="meta">
      <span>Module: Vision</span>
      <span>Course: DATA 201</span>
    </div>
    <div class="resources">
      <a href="lecture.ipynb" class="btn">Download Notebook</a>
      <a href="lecture.md" class="btn btn-secondary">View Markdown</a>
    </div>
  </header>

  <section class="objectives">
    <h2>Learning Objectives</h2>
    <ol>
      <li>Understand dimensionality reduction (PCA, t-SNE)</li>
      <li>Implement autoencoders for non-linear feature learning</li>
      <li>Visualize learned representations in latent space</li>
      <li>Apply pre-trained deep learning models</li>
      <li>Understand the evolution to modern vision AI</li>
    </ol>
  </section>

  <section class="content">
    <h2>Core Idea</h2>
    <blockquote>
      <strong>The right representation makes classification trivial.</strong><br>
      Instead of engineering features, learn a transformation where classes separate naturally.
    </blockquote>

    <h2>Part 1: Dimensionality Reduction</h2>

    <h3>The Problem</h3>
    <ul>
      <li>Each 8×8 image = 64-dimensional vector</li>
      <li>Digits don't span all of $\mathbb{R}^{64}$</li>
      <li>They lie on a lower-dimensional <strong>manifold</strong></li>
    </ul>

    <h3>PCA (Principal Component Analysis)</h3>
    <p>Find directions of maximum variance:</p>
    <ol>
      <li>Center data: $\tilde{X} = X - \mu$</li>
      <li>Compute covariance: $\Sigma = \frac{1}{n} \tilde{X}^T \tilde{X}$</li>
      <li>Eigendecomposition: $\Sigma v_i = \lambda_i v_i$</li>
      <li>Project: $Z = \tilde{X} V_k$</li>
    </ol>
    <p>PCA is <strong>linear</strong> - finds the best linear projection.</p>

    <h3>t-SNE</h3>
    <p>Non-linear: preserves <strong>local neighborhoods</strong>.</p>
    <p>Minimize: $\text{KL}(P || Q) = \sum_{ij} P_{ij} \log \frac{P_{ij}}{Q_{ij}}$</p>

    <h2>Part 2: Autoencoders</h2>
    <p>Neural network that learns to compress and reconstruct:</p>
    <pre>Input (64D) → Encoder → Bottleneck (2D) → Decoder → Output (64D)</pre>

    <h3>Training Objective</h3>
    <p>Minimize reconstruction error:</p>
    <p>$$\mathcal{L} = \frac{1}{n} \sum_{i=1}^n ||x_i - \hat{x}_i||^2$$</p>

    <h3>Why It Works</h3>
    <ul>
      <li>Forces bottleneck to learn compressed representation</li>
      <li>Non-linear activations capture complex patterns</li>
      <li>Can interpolate between examples in latent space</li>
    </ul>

    <h2>Part 3: Modern Deep Learning</h2>

    <h3>The 2012 Revolution (AlexNet)</h3>
    <ul>
      <li>Won ImageNet with 84.6% accuracy (vs 73.8% previous)</li>
      <li>Convolutional Neural Networks (CNNs)</li>
      <li>Sparked the modern deep learning era</li>
    </ul>

    <h3>CNN Key Ideas</h3>
    <ul>
      <li><strong>Local connectivity</strong>: Each neuron looks at small patch</li>
      <li><strong>Weight sharing</strong>: Same filter across image</li>
      <li><strong>Hierarchical</strong>: Edges → Textures → Parts → Objects</li>
    </ul>

    <h3>Modern Vision AI (2021-2024)</h3>
    <ul>
      <li><strong>CLIP</strong>: Vision + language understanding</li>
      <li><strong>SAM</strong>: Universal segmentation</li>
      <li><strong>DALL-E, Stable Diffusion</strong>: Text-to-image</li>
      <li><strong>GPT-4V</strong>: Multimodal understanding</li>
    </ul>

    <h2>The Paradigm Shift</h2>
    <table>
      <tr><th>Traditional CV</th><th>Modern ML/DL</th></tr>
      <tr><td>Hand-coded features</td><td>Learned features</td></tr>
      <tr><td>Domain expertise required</td><td>Data-driven</td></tr>
      <tr><td>Linear transformations</td><td>Non-linear hierarchies</td></tr>
      <tr><td>~70-80% accuracy</td><td>~95-99%+ accuracy</td></tr>
    </table>

    <h2>Key Takeaway</h2>
    <blockquote>
      <strong>Intelligence emerges from learning the right representation.</strong><br>
      We don't tell machines how to see. We give them data and let them figure out what matters.
    </blockquote>
  </section>

  <footer class="lecture-nav">
    <a href="../how-machines-see-1/lecture.html" class="prev">← Part 1: Traditional CV</a>
    <span></span>
  </footer>
</article>

<style>
.breadcrumb { font-size: 0.9rem; color: #666; margin-bottom: 1.5rem; }
.breadcrumb a { color: var(--accent-color); }
.lecture-content header { margin-bottom: 2rem; padding-bottom: 1.5rem; border-bottom: 1px solid var(--border-color); }
.lecture-content h1 { margin-bottom: 0.25rem; }
.subtitle { font-size: 1.3rem; color: #666; margin-bottom: 1rem; }
.meta { font-size: 0.9rem; color: #888; margin-bottom: 1rem; }
.meta span { margin-right: 1.5rem; }
.btn-secondary { background: #e9ecef; color: var(--primary-color); }
.objectives { background: var(--light-bg); padding: 1.5rem; border-radius: 6px; margin-bottom: 2rem; }
.objectives h2 { margin-top: 0; }
.lecture-nav { display: flex; justify-content: space-between; margin-top: 3rem; padding-top: 1.5rem; border-top: 1px solid var(--border-color); }
pre { background: #f4f4f4; padding: 1rem; border-radius: 4px; overflow-x: auto; }
</style>
